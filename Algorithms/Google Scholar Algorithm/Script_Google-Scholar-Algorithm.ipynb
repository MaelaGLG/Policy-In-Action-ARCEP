{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a63c2ff7",
   "metadata": {},
   "source": [
    "\n",
    "# Collection of Articles using Google Scholar\n",
    "\n",
    "### Author : Maela Guillaume-Le Gall\n",
    " \n",
    "------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Ce code effectue une recherche automatisée sur Google Scholar en utilisant les paramètres définis dans la première section **\"Configuration\"**. L'algorithme calcule la pertinence des articles en identifiant les mots communs entre la requête et le titre de l'article. Les articles sont ensuite triés, et le score de pertinence est combiné avec l'année de publication et le nombre de citations pour déterminer l'ordre des résultats. Pour chaque article sélectionné, l'algorithme webscrape et nettoie le texte complet de l'abstract via Selenium. Finalement, **les résultats sont exportés sous forme de tableau dans un fichier Excel téléchargeable à la fin du document.**\n",
    "\n",
    "Les paramètres de **Configuration** peuvent être facilement modifiés dans la première cellule de code :\n",
    "- requête = **QUERY**\n",
    "- nombre d'articles à récupérer = **NUM_FETCH** \n",
    "- année minimale = **MIN_YEAR** \n",
    "- nombre d'articles à exporter = **NUM_SELECT**. \n",
    "\n",
    "*Le nombre d'articles à récupérer doit toujours être supérieur au nombre d'articles à exporter, car certains articles peuvent être exclus s'ils sont antérieurs à la date minimale (MIN_YEAR) ou en raison de mécanismes anti-bot. Il est recommandé d'indiquer un NUM_FETCH supérieur d'au moins un tiers à NUM_SELECT.*\n",
    "\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "This code performs an automated search on Google Scholar using the parameters defined in the first **\"Configuration\"** section. The algorithm calculates the relevance of articles by identifying the common words between the query and the article title. The articles are then sorted, and the relevance score is combined with the publication year and the number of citations to determine the order of the results. For each selected article, the algorithm webscrapes and cleans the full abstract text using Selenium. Finally, **the results are exported as a table in a Excel file downloadable at the end of the document.**\n",
    "\n",
    "The **Configuration** parameters can be easily modified in the first code cell:\n",
    "- query = **QUERY**\n",
    "- number of articles to fetch = **NUM_FETCH**\n",
    "- minimum publication year = **MIN_YEAR**\n",
    "- number of articles to export = **NUM_SELECT**\n",
    "\n",
    "*The number of articles to fetch should always be greater than the number of articles to export, as some articles may be excluded if they are older than the minimum date (MIN_YEAR) or due to anti-bot mechanisms. It is recommended to set NUM_FETCH to at least one third more than NUM_SELECT.*\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5272e0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================================================\n",
    "# CONFIGURATION \n",
    "# ==============================================================================================================\n",
    "\n",
    "QUERY = \"Environmental Impacts Artificial Intelligence\"\n",
    "# Number of articles to fetch from Google Scholar before filtering\n",
    "NUM_FETCH = 15\n",
    "# Minimum publication year to consider an article valid \n",
    "MIN_YEAR = 2020\n",
    "# Number of valid articles to select for export\n",
    "NUM_SELECT = 10\n",
    "\n",
    "\n",
    "# Warning : The number of articles to fetch should always be greater than the number of articles to export, \n",
    "#as some articles may be excluded if they are older than the minimum date (MIN_YEAR) or due to anti-bot mechanisms. \n",
    "#It is recommended to set NUM_FETCH to at least one third more than NUM_SELECT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68de6da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================================================\n",
    "# INSTALL PACKAGES AND IMPORT LIBRARIES\n",
    "# ==============================================================================================================\n",
    "\n",
    "# Install required packages if not already installed\n",
    "import importlib.util\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "required_packages = [\"pandas\", \"scholarly\", \"selenium\", \"chromedriver_autoinstaller\"]\n",
    "for package in required_packages:\n",
    "    if importlib.util.find_spec(package) is None:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "\n",
    "# Import libraries\n",
    "import re\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "\n",
    "from scholarly import scholarly\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fa176f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ==============================================================================================================\n",
    "\n",
    "def clean_abstract(text):\n",
    "    \"\"\"\n",
    "    Cleans the abstract text by removing unwanted lines.\n",
    "    - Removes the word \"abstract\" at the beginning.\n",
    "    - If a line starts with \"Highlights\", skips subsequent lines until a line starting with \"abstract\" is encountered (which is kept).\n",
    "    - Removes lines starting with \"Keywords:\", \"Download\", \"Graphical abstract\", \"Fig.\", \"Table\", \"Cookies\", etc.\n",
    "    - Removes any line containing \"Cite this article\".\n",
    "    - Stops processing if a line starts with \"This is a preview\".\n",
    "    \"\"\"\n",
    "    # Remove \"abstract\" if it appears at the beginning\n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    skip_until_abstract = False\n",
    "\n",
    "    for line in lines:\n",
    "        if skip_until_abstract:\n",
    "            if re.match(r'^\\s*abstract', line, re.IGNORECASE):\n",
    "                skip_until_abstract = False\n",
    "                cleaned_lines.append(line.strip())\n",
    "            continue\n",
    "\n",
    "        if re.match(r'^\\s*Highlights', line, re.IGNORECASE):\n",
    "            skip_until_abstract = True\n",
    "            continue\n",
    "\n",
    "        if re.match(r'^\\s*(Keywords:|Download|Graphical abstract)', line, re.IGNORECASE):\n",
    "            continue\n",
    "\n",
    "        if re.match(r'^\\s*(Fig\\.|Table|Cookies|Cookie Settings|©|All content on this site)', line, re.IGNORECASE):\n",
    "            continue\n",
    "        \n",
    "        if re.search(r'(^\\s*Please note,)|(^Your institution has not purchased)|(^You are not authenticated)|Cite this article', line, re.IGNORECASE):\n",
    "            break\n",
    "\n",
    "        cleaned_lines.append(line.strip())\n",
    "\n",
    "    return \"\\n\".join(cleaned_lines).strip()\n",
    "\n",
    "\n",
    "def get_full_abstract(url, driver):\n",
    "    \"\"\"\n",
    "    Retrieves the full abstract using a dictionary of generic selectors for different websites.\n",
    "    If no specific selector works, a generic method (iterating over paragraphs) is used.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "\n",
    "        # Try to close the cookie banner if present\n",
    "        try:\n",
    "            cookie_button = WebDriverWait(driver, 5).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \n",
    "                    \"//button[contains(text(),'Accept') or contains(text(),'Close') or contains(text(),'I agree')]\"))\n",
    "            )\n",
    "            cookie_button.click()\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        domain = urlparse(url).netloc.lower()\n",
    "        abstract_text = \"\"\n",
    "\n",
    "        # Dictionary of generic selectors by domain\n",
    "        selectors = {\n",
    "            \"mdpi.com\": [\n",
    "                (\"css\", \"div#art-abstract\"),\n",
    "                (\"css\", \"section.abstract\"),\n",
    "                (\"css\", \"div.art-abstract\")\n",
    "            ],\n",
    "            \"sciencedirect.com\": [\n",
    "                (\"css\", \"section#abstract\"),\n",
    "                (\"css\", \"section#abstracts\"),\n",
    "                (\"css\", \"div.Abstracts\")\n",
    "            ],\n",
    "            \"springer.com\": [\n",
    "                (\"xpath\", \"//section[contains(@class,'Abstract')]\"),\n",
    "                (\"xpath\", \"//div[contains(@class,'c-article-section__content')]\")\n",
    "            ],\n",
    "            \"arxiv.org\": [\n",
    "                (\"css\", \"blockquote.abstract\"),\n",
    "                (\"css\", \"blockquote.abstract.mathjax\")\n",
    "            ],\n",
    "            \"nature.com\": [\n",
    "                (\"xpath\", \"//div[contains(@class,'Abstract')]\"),\n",
    "                (\"xpath\", \"//section[contains(@class,'abstract')]\")\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        found = False\n",
    "        for key, sel_list in selectors.items():\n",
    "            if key in domain:\n",
    "                for sel_type, selector in sel_list:\n",
    "                    try:\n",
    "                        if sel_type == \"css\":\n",
    "                            elem = WebDriverWait(driver, 5).until(\n",
    "                                EC.visibility_of_element_located((By.CSS_SELECTOR, selector))\n",
    "                            )\n",
    "                        else:\n",
    "                            elem = WebDriverWait(driver, 5).until(\n",
    "                                EC.visibility_of_element_located((By.XPATH, selector))\n",
    "                            )\n",
    "                        abstract_text = elem.text\n",
    "                        found = True\n",
    "                        break\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                if found:\n",
    "                    break\n",
    "\n",
    "        if not abstract_text:\n",
    "            paragraphs = driver.find_elements(By.TAG_NAME, 'p')\n",
    "            for para in paragraphs:\n",
    "                text_para = para.text.strip()\n",
    "                if text_para and len(text_para.split()) > 5:\n",
    "                    abstract_text += text_para + \"\\n\"\n",
    "            \n",
    "            # >>> Remove duplicate lines <<<\n",
    "            lines = abstract_text.splitlines()\n",
    "            unique_lines = []\n",
    "            for line in lines:\n",
    "                line_stripped = line.strip()\n",
    "                if line_stripped and line_stripped not in unique_lines:\n",
    "                    unique_lines.append(line_stripped)\n",
    "            abstract_text = \"\\n\".join(unique_lines)\n",
    "        \n",
    "        # Final cleaning of the abstract text\n",
    "        abstract_text = clean_abstract(abstract_text)\n",
    "        \n",
    "        if not abstract_text.strip():\n",
    "            return \"No abstract found\"\n",
    "        return abstract_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error fetching abstract: {str(e)}\"\n",
    "\n",
    "\n",
    "def relevance_score(title, query):\n",
    "    \"\"\"\n",
    "    Computes a relevance score by comparing the words in the title with those in the query.\n",
    "    \"\"\"\n",
    "    title_words = re.findall(r'\\w+', title.lower())\n",
    "    query_words = re.findall(r'\\w+', query.lower())\n",
    "    return len(set(title_words).intersection(query_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7236cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the list of relevant articles on your subject, download it by clicking\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='Relevant Articles IA.xlsx' target='_blank'>Relevant Articles IA.xlsx</a><br>"
      ],
      "text/plain": [
       "C:\\Users\\maela\\Relevant Articles IA.xlsx"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==============================================================================================================\n",
    "# SELECTION AND EXPORTATION OF THE TABLE\n",
    "# ==============================================================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the Selenium WebDriver (using default settings)\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # Start the search on Google Scholar using the configured query\n",
    "    search_query = scholarly.search_pubs(QUERY)\n",
    "    articles = []\n",
    "    for i in range(NUM_FETCH):\n",
    "        try:\n",
    "            article = next(search_query)\n",
    "            articles.append(article)\n",
    "        except StopIteration:\n",
    "            break\n",
    "\n",
    "    # Process each article: compute relevance score, convert publication year and citation count to integers\n",
    "    for article in articles:\n",
    "        title = article.get('bib', {}).get('title', '')\n",
    "        article['relevance'] = relevance_score(title, QUERY)\n",
    "        try:\n",
    "            article['year'] = int(article.get('bib', {}).get('pub_year', 0))\n",
    "        except:\n",
    "            article['year'] = 0\n",
    "        try:\n",
    "            article['citations'] = int(article.get('num_citations', 0))\n",
    "        except:\n",
    "            article['citations'] = 0\n",
    "\n",
    "    # Filter articles with a publication year >= MIN_YEAR and sort them by relevance, year, and citation count\n",
    "    sorted_articles = sorted(\n",
    "        [a for a in articles if a.get('year', 0) >= MIN_YEAR],\n",
    "        key=lambda a: (a['relevance'], a['year'], a['citations']),\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    data = []\n",
    "    valid_count = 0\n",
    "\n",
    "    # Loop through the sorted articles until NUM_SELECT valid articles are collected\n",
    "    for article in sorted_articles:\n",
    "        title = article['bib'].get('title', 'N/A')\n",
    "        authors = article['bib'].get('author', 'N/A')\n",
    "        if isinstance(authors, list):\n",
    "            authors = \", \".join(authors)\n",
    "        year = article.get('year', 'N/A')\n",
    "        citations = article.get('citations', 'N/A')\n",
    "        relevance = article.get('relevance', 'N/A')\n",
    "        pub_url = article.get('pub_url', '')\n",
    "        \n",
    "        if pub_url:\n",
    "            full_abstract = get_full_abstract(pub_url, driver)\n",
    "        else:\n",
    "            full_abstract = \"No URL provided\"\n",
    "        \n",
    "        # Remove the word \"Abstract\" at the beginning of the abstract if present\n",
    "        full_abstract = re.sub(r'^\\s*Abstract[:]*\\s*', '', full_abstract, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Skip the article if the full abstract is not available or contains a blocking message\n",
    "        if full_abstract.strip() in [\n",
    "            \"No abstract found\", \n",
    "            \"Confirmez que vous êtes un humain en effectuant l’action ci-dessous.\"\n",
    "        ]:\n",
    "            continue\n",
    "        \n",
    "        data.append({\n",
    "            \"Article\": f\"Article {valid_count + 1}\",\n",
    "            \"Title\": title,\n",
    "            \"Author(s)\": authors,\n",
    "            \"Year\": year,\n",
    "            \"Citations\": citations,\n",
    "            \"Relevance Score\": relevance,\n",
    "            \"Full Abstract\": full_abstract,\n",
    "            \"URL\": pub_url\n",
    "        })\n",
    "        \n",
    "        valid_count += 1\n",
    "        if valid_count >= NUM_SELECT:\n",
    "            break\n",
    "\n",
    "    # Create a DataFrame from the collected data and export it to an Excel file\n",
    "    df = pd.DataFrame(data, columns=[\"Title\", \"Author(s)\", \"Year\", \"Citations\", \"Relevance Score\", \"Full Abstract\", \"URL\"])\n",
    "    df.to_excel(\"Relevant Articles IA.xlsx\", index=False)\n",
    "    print(\"Here is the list of relevant articles on your subject, download it by clicking\")\n",
    "\n",
    "    # Generate a download link (for Jupyter Notebook)\n",
    "    from IPython.display import FileLink\n",
    "    display(FileLink(\"Relevant Articles IA.xlsx\"))\n",
    "\n",
    "    # Close the Selenium WebDriver after export\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5b7377",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
